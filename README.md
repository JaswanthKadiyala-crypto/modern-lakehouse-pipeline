# ğŸ—ï¸ Modern Lakehouse Pipeline

> **Production-ready data platform:** Ingest IoT streaming data â†’ Real-time processing â†’ Anomaly detection â†’ Predictive forecasting â†’ Beautiful dashboards. Built to demonstrate how companies like Netflix, Databricks, and Uber build data systems.

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![Spark](https://img.shields.io/badge/Spark-3.5-orange.svg)](https://spark.apache.org/)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED.svg)](https://www.docker.com/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## ğŸ¯ The Problem This Solves

**Real-world challenge:** Companies have data in many places (IoT sensors, APIs, files), but can't easily:
- âŒ Process it all together
- âŒ Find patterns or anomalies
- âŒ Make predictions
- âŒ Monitor data quality
- âŒ Scale without breaking

**The Solution:** This project demonstrates a complete lakehouse that:
- âœ… **Ingests** from multiple sources (streaming + batch)
- âœ… **Processes** massive data with Spark (handles terabytes)
- âœ… **Analyzes** with SQL using dbt (familiar to analysts)
- âœ… **Detects** anomalies with ML (no broken sensors!)
- âœ… **Predicts** future metrics (capacity planning)
- âœ… **Automates** everything daily with Airflow (runs while you sleep)

**Real-world scenario:** IoT device monitoring with **predictive maintenance** â€” detect broken equipment before expensive downtime.

---

## ğŸ’¼ Why This Matters (Business Value)

| Problem | This Project Shows | Business Impact |
|---------|-------------------|-----------------|
| Data scattered everywhere | Unified data lake | Single source of truth |
| Reports take weeks | dbt transforms in minutes | Faster decisions |
| No early warning for failures | ML anomaly detection | Prevent equipment failure |
| Can't forecast demand | LightGBM predictions | Optimize resources |
| Pipeline breaks silently | Airflow monitoring | Reliable automation |
| Not scalable | Spark + Iceberg | Handle 10x data growth |

---

## ğŸ‘¥ Who Should Use This

### For Interviews/Portfolios
```
âœ… Data Engineers    - Shows you can design complete pipelines
âœ… ML Engineers      - Shows you understand MLOps & deployment
âœ… Analytics Eng     - Shows you know modern data stacks
âœ… Platform Eng      - Shows you can orchestrate systems
âœ… Startup CTOs      - "This is exactly what we need"
```

### Real Companies Using Similar Stacks
- **Uber** - Analytics on billions of ride events (Spark + Iceberg)
- **Netflix** - Streaming pipelines with ML predictions
- **Databricks** - Built Lakehouse concept (Iceberg + Delta)
- **Airbnb** - Orchestration at scale (Airflow)
- **Stripe** - Real-time fraud detection (Spark ML)

---

## ğŸ“ What You Learn By Building This

**Data Engineering Fundamentals**
```
âœ… How to ingest data from multiple sources
âœ… Real-time vs batch processing tradeoffs
âœ… Data quality & validation patterns
âœ… Schema design for analytics
âœ… Table format architecture (Iceberg vs Delta)
```

**ML + Analytics**
```
âœ… Feature engineering at scale
âœ… Model versioning & experiment tracking
âœ… Deploying predictions back to data warehouse
âœ… Monitoring model performance
âœ… A/B testing infrastructure
```

**DevOps + Reliability**
```
âœ… Infrastructure as Code (Docker)
âœ… Service orchestration (docker-compose)
âœ… Error handling & retries
âœ… Monitoring & alerting patterns
âœ… Production-grade logging
```

---

## ğŸ›ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA SOURCES                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ IoT Streaming Events (JSON)  â€¢ Batch Files (CSV/Parquet)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   INGESTION LAYER (Python)                       â”‚
â”‚          Upload to MinIO/S3 â†’ raw/events/ & raw/files/          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PROCESSING LAYER (Apache Spark)                     â”‚
â”‚        Bronze (raw) â†’ Silver (cleaned) â†’ Gold (curated)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                         â”‚
           â–¼                         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ICEBERG TABLES  â”‚      â”‚  DELTA TABLES    â”‚
    â”‚ (Analytics)     â”‚      â”‚  (ML Features)   â”‚
    â”‚                 â”‚      â”‚                  â”‚
    â”‚â€¢ device_metrics â”‚      â”‚â€¢ ml_features     â”‚
    â”‚â€¢ device_events  â”‚      â”‚â€¢ training_data   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                         â”‚
             â–¼                         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   SNOWFLAKE      â”‚      â”‚  ML PIPELINE    â”‚
    â”‚ (Data Warehouse) â”‚      â”‚  (MLflow)       â”‚
    â”‚                  â”‚      â”‚                 â”‚
    â”‚ â€¢ External Tablesâ”‚      â”‚ â€¢ Anomaly       â”‚
    â”‚ â€¢ dbt Marts:     â”‚      â”‚   Detection     â”‚
    â”‚   - device_healthâ”‚â—„â”€â”€â”€â”€â”€â”¤ â€¢ Forecasting   â”‚
    â”‚   - anomalies    â”‚  â”Œâ”€â”€â–ºâ”‚   (predictions) â”‚
    â”‚   - predictions  â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚   - sla_metrics  â”‚  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
               â”‚          â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–²
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  ORCHESTRATION (Airflow)      â”‚
        â”‚                               â”‚
        â”‚ DAG: lakehouse_end_to_end    â”‚
        â”‚ Schedule: Daily @ 2 AM        â”‚
        â”‚ Monitoring & Alerts           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Technology Stack

| Category | Technologies |
|----------|-------------|
| **Languages** | Python 3.11, SQL, YAML |
| **Data Processing** | Apache Spark 3.5 (PySpark) |
| **Table Formats** | Apache Iceberg 1.4, Delta Lake 3.0 |
| **Data Warehouse** | Snowflake |
| **Analytics Engineering** | dbt Core 1.7 |
| **ML Framework** | Scikit-learn, LightGBM |
| **ML Tracking** | MLflow 2.9 |
| **Orchestration** | Apache Airflow 2.8 |
| **Object Storage** | MinIO (S3-compatible) |
| **Infrastructure** | Docker Compose |
| **Code Quality** | Black, Flake8, pytest |

---

## ğŸ“ Project Structure

```
modern-lakehouse-pipeline/
â”‚
â”œâ”€â”€ README.md                          # You are here ğŸ‘ˆ
â”œâ”€â”€ Makefile                           # One-command setup & operations
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”‚
â”œâ”€â”€ ğŸ“ infra/                          # Infrastructure setup
â”‚   â”œâ”€â”€ docker-compose.yml             # All services (Airflow, MinIO, Spark, MLflow)
â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â”œâ”€â”€ Dockerfile                 # Airflow container
â”‚   â”‚   â””â”€â”€ requirements.txt            # Python dependencies
â”‚   â””â”€â”€ spark/
â”‚       â”œâ”€â”€ Dockerfile                 # Spark container
â”‚       â””â”€â”€ spark-defaults.conf         # Spark configuration
â”‚
â”œâ”€â”€ ğŸ“ ingestion/                      # Data ingestion scripts
â”‚   â”œâ”€â”€ generator_streaming.py         # IoT event simulator
â”‚   â”œâ”€â”€ upload_files.py                # Batch file uploader (CSV/Parquet)
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ ğŸ“ spark_jobs/                     # Spark ETL pipelines
â”‚   â”œâ”€â”€ bronze_to_silver.py            # Data cleaning & validation
â”‚   â”œâ”€â”€ silver_to_iceberg.py           # Write Iceberg tables
â”‚   â”œâ”€â”€ silver_to_delta.py             # Write Delta tables
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ ğŸ“ snowflake/                      # Snowflake setup
â”‚   â”œâ”€â”€ setup.sql                      # External stages & integrations
â”‚   â”œâ”€â”€ ddl.sql                        # Table definitions
â”‚   â””â”€â”€ copy_into.sql                  # Data loading scripts
â”‚
â”œâ”€â”€ ğŸ“ dbt_snowflake/                  # dbt transformations
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_iot_events.sql
â”‚   â”‚   â”‚   â””â”€â”€ stg_device_files.sql
â”‚   â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â”‚   â””â”€ int_device_health.sql
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â”‚       â”œâ”€â”€ mart_device_daily_health.sql
â”‚   â”‚       â”œâ”€â”€ mart_device_anomalies.sql
â”‚   â”‚       â”œâ”€â”€ mart_device_predictions.sql
â”‚   â”‚       â””â”€â”€ mart_ingestion_sla.sql
â”‚   â”œâ”€â”€ tests/
â”‚   â””â”€â”€ packages.yml
â”‚
â”œâ”€â”€ ğŸ“ ml/                             # Machine Learning pipelines
â”‚   â”œâ”€â”€ train_anomaly.py               # Anomaly detection (IsolationForest)
â”‚   â”œâ”€â”€ train_forecast.py              # Forecasting (LightGBM)
â”‚   â”œâ”€â”€ feature_pipeline.py            # Feature engineering
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ ğŸ“ dags/                           # Airflow DAGs
â”‚   â””â”€â”€ lakehouse_end_to_end.py        # Main orchestration DAG
â”‚
â””â”€â”€ ğŸ“ docs/                           # Documentation
    â”œâ”€â”€ architecture.md                # Detailed architecture
    â”œâ”€â”€ data_flow.md                   # Data flow explanation
    â””â”€â”€ screenshots/                   # Deployment evidence
```

---

## ğŸš€ Quick Start (10 Minutes)

### Prerequisites

- **Docker Desktop** (installed and running)
- **Python 3.11+**
- **Git** 
- **8GB RAM minimum**

### Step 1: Clone the Repository

```bash
git clone https://github.com/JaswanthKadiyala-crypto/modern-lakehouse-pipeline.git
cd modern-lakehouse-pipeline
```

### Step 2: Start Infrastructure

```bash
# Using Docker Compose (all services start automatically)
docker-compose -f infra/docker-compose.yml up -d

# Wait 2-3 minutes for services to initialize
# Check logs:
docker-compose -f infra/docker-compose.yml logs -f
```

### Step 3: Access the Services

| Service | URL | Credentials |
|---------|-----|------------|
| **Airflow** | http://localhost:8080 | `airflow` / `airflow` |
| **MinIO (S3)** | http://localhost:9000 | `minioadmin` / `minioadmin` |
| **MLflow** | http://localhost:5000 | - |
| **Spark Master UI** | http://localhost:8888 | - |

### Step 4: Run the Pipeline

```bash
# Option 1: Using Makefile (recommended)
make run

# Option 2: Trigger manually in Airflow UI
# 1. Go to http://localhost:8080
# 2. Find DAG: "lakehouse_end_to_end"
# 3. Click "Trigger DAG"
```

### Step 5: View Results

```bash
# Check data ingested to MinIO
aws s3 ls s3://lakehouse/raw/ \
  --endpoint-url http://localhost:9000

# Query Snowflake (if connected)
snowsql -q "SELECT * FROM marts.device_health LIMIT 10;"
```

---

## ğŸ“ What This Demonstrates

### Data Engineering Skills
âœ… Multi-source ingestion (streaming + batch)  
âœ… Lakehouse architecture (Iceberg + Delta)  
âœ… Medallion pattern (Bronze â†’ Silver â†’ Gold)  
âœ… Schema evolution & data quality checks  

### Analytics Engineering Skills
âœ… dbt modeling (staging â†’ intermediate â†’ marts)  
âœ… SQL transformations with testing  
âœ… Lineage & documentation  

### ML Engineering Skills
âœ… Feature engineering from data lake  
âœ… Model training (classification + regression)  
âœ… Experiment tracking with MLflow  
âœ… Model deployment (predictions back to warehouse)  

### DevOps & Orchestration
âœ… Infrastructure as Code (Docker Compose)  
âœ… Workflow orchestration (Airflow DAGs)  
âœ… Monitoring & alerting  
âœ… Reproducible environments  

---

## ğŸ—ºï¸ Development Roadmap

### âœ… Phase 1: Foundation (You are here!)
- [x] Project structure
- [x] README & documentation
- [x] Docker Compose setup

### ğŸš§ Phase 2: Core Pipeline (Next)
- [ ] Data ingestion scripts
- [ ] Spark ETL jobs
- [ ] Table format configuration

### ğŸ“‹ Phase 3: Analytics
- [ ] Snowflake integration
- [ ] dbt models & tests
- [ ] Analytics marts

### ğŸ¤– Phase 4: ML Pipeline
- [ ] Feature engineering
- [ ] Model training
- [ ] MLflow tracking

### â° Phase 5: Orchestration
- [ ] Airflow DAG setup
- [ ] Monitoring & alerts
- [ ] Error handling

### ğŸš€ Phase 6: Deployment
- [ ] CI/CD pipeline (GitHub Actions)
- [ ] Kubernetes deployment
- [ ] Production hardening

---

## ğŸ‘¤ Author

**Jaswanth Kadiyala**

- ğŸ’¼ GitHub: [@JaswanthKadiyala-crypto](https://github.com/JaswanthKadiyala-crypto)
- ğŸ“§ Email: Jaswanth.Kadiyala@gmail.com

---

## ğŸ“„ License

MIT License - feel free to use this project for learning and portfolio purposes!

---

## âš¡ The README.md Difference

### WITHOUT a Good README
```
âŒ "What is this project?" - No context
âŒ "How do I run it?" - No instructions  
âŒ "Why should I care?" - No value prop
âŒ GitHub tab closes - Potential connection missed
âŒ Looks unprofessional - Auto-rejected
```

**Result:** Your code never gets seen

### WITH This README
```
âœ… "Oh! It's a modern data lakehouse" - Instant clarity
âœ… "One command: make setup" - Easy to try
âœ… "Companies like Netflix use this" - Credibility
âœ… "Recruiter reads more" - Engagement
âœ… "This person knows data stack" - Interview scheduled
```

**Result:** Your skills get recognized

---

â­ **If you found this helpful, please star this repository!** â­