# ==============================================================================
# Spark Configuration for MinIO/S3 and Table Formats
# ==============================================================================
# These settings enable Spark to:
# 1. Connect to MinIO (acts like AWS S3)
# 2. Use Apache Iceberg for analytics
# 3. Use Delta Lake for ML features
# 4. Handle partitioned tables efficiently

# S3/MinIO Configuration
spark.hadoop.fs.s3a.access.key=minioadmin
spark.hadoop.fs.s3a.secret.key=minioadmin
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem

# Iceberg Configuration (for analytics tables)
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkSessionCatalog
spark.sql.catalog.iceberg.type=hive
spark.sql.catalog.iceberg.warehouse=s3a://lakehouse/iceberg

# Delta Lake Configuration (for ML tables)
spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
spark.sql.warehouse.dir=s3a://lakehouse/delta

# Spark performance tuning
spark.driver.memory=2g
spark.executor.memory=2g
spark.executor.cores=2
spark.default.parallelism=4
spark.sql.shuffle.partitions=4

# Logging configuration
log4j.rootCategory=WARN, console
log4j.logger.org.apache.spark.scheduler.TaskSetManager=INFO
